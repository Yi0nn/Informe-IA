{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>💟INFORME MACHING LEARNING💟</center>\n",
    "\n",
    "El objetivo de este informe es crear dos notebooks. Uno donde se utilice la técnica de redes neuronales\n",
    "y otro para la técnica de árboles de decisión, todo esto para determinar los hiperparámetros óptimos para obtener la mayor precisión posible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💟INTEGRANTES💟\n",
    "* Isabela Rosero 2128720\n",
    "* Luisa Cardenas 1823494 \n",
    "* Stefhania Noguera 2125854"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💟COMO CORRER EL ARCHIVO💟\n",
    "* Instalar pandas con *pip install pandas*\n",
    "* Instalar sklearn con *pip install  scikit-learn*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💟IMPORTACIONES💟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar bibliotecas necesarias\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💟LEER EL ARCHIVO CSV💟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Leer el archivo breastCancer.csv.\n",
    "data = pd.read_csv('breastCancer.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💟SELECCIONAR EL 80% DEL CONJUNTO DE DATOS Y EL 20 PARA PRUEBAS💟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Seleccionar aleatoriamente el 80% del conjunto de datos para entrenar y el 20% restante para las pruebas.\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💟NORMALIZAR LOS DATOS💟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Utilizar una estrategia para normalizar los datos.\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(train_data.iloc[:, :-1])  # Excluir la columna de diagnóstico\n",
    "y_train = train_data['diagnosis']\n",
    "\n",
    "X_test = scaler.transform(test_data.iloc[:, :-1])\n",
    "y_test = test_data['diagnosis']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💟CONSTRUIR 5 REDES CON VARIACIONES💟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Activation Solver Hidden Layer Sizes  Accuracy    Confusion Matrix\n",
      "0       relu   adam         (200, 300)  0.991228  [[73, 0], [1, 40]]\n",
      "1       tanh  lbfgs       (10, 20, 10)  0.938596  [[70, 3], [4, 37]]\n",
      "2   logistic    sgd       (20, 15, 10)  0.640351  [[73, 0], [41, 0]]\n",
      "3   identity   adam     (10, 10, 5, 5)  0.973684  [[73, 0], [3, 38]]\n",
      "4       tanh   adam          (150, 75)  0.964912  [[72, 1], [3, 38]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 4. Construir 5 redes neuronales variando la función de activación, el solver, y la cantidad de capas ocultas y de neuronas por cada capa oculta.\n",
    "results = []\n",
    "\n",
    "# Seleccionar combinaciones específicas\n",
    "combinations = [\n",
    "    ('relu', 'adam', (200 , 300)),\n",
    "    ('tanh', 'lbfgs', (10, 20, 10)),\n",
    "    ('logistic', 'sgd', (20, 15, 10)),\n",
    "    ('identity', 'adam', (10, 10, 5, 5)),\n",
    "    ('tanh', 'adam', (150, 75)),\n",
    "]\n",
    "\n",
    "for activation, solver, hidden_layer_size in combinations:\n",
    "    # Crear y entrenar el modelo\n",
    "    model = MLPClassifier(activation=activation, solver=solver, hidden_layer_sizes=hidden_layer_size, random_state=123, max_iter=500)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Hacer predicciones\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calcular accuracy y matriz de confusión\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Guardar resultados\n",
    "    results.append({\n",
    "        'Activation': activation,\n",
    "        'Solver': solver,\n",
    "        'Hidden Layer Sizes': hidden_layer_size,\n",
    "        'Accuracy': acc,\n",
    "        'Confusion Matrix': conf_matrix\n",
    "    })\n",
    "\n",
    "# Mostrar resultados en una tabla\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💟Variaciones con hyperparametro alpha💟\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Se varia el hyperparametro de alpha\n",
      "  Activation Solver Hidden Layer Sizes  Alpha  Accuracy    Confusion Matrix\n",
      "0       relu   adam         (200, 300)    0.3  0.964912  [[72, 1], [3, 38]]\n",
      "1       relu   adam         (200, 300)    0.1  0.982456  [[72, 1], [1, 40]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\")\n",
    "print(\"Se varia el hyperparametro de alpha\")\n",
    "# Entrenar primera red con alpha=0.3\n",
    "model_1 = MLPClassifier(activation='relu', solver='adam', hidden_layer_sizes=(200, 300), alpha=0.3, random_state=123, max_iter=500)\n",
    "model_1.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones para la primera red\n",
    "y_pred_1 = model_1.predict(X_test)\n",
    "\n",
    "# Calcular accuracy y matriz de confusión para la primera red\n",
    "acc_1 = accuracy_score(y_test, y_pred_1)\n",
    "conf_matrix_1 = confusion_matrix(y_test, y_pred_1)\n",
    "\n",
    "# Guardar resultados de la primera red\n",
    "results_1 = {\n",
    "    'Activation': 'relu',\n",
    "    'Solver': 'adam',\n",
    "    'Hidden Layer Sizes': (200, 300),\n",
    "    'Alpha': 0.3,\n",
    "    'Accuracy': acc_1,\n",
    "    'Confusion Matrix': conf_matrix_1\n",
    "}\n",
    "\n",
    "# Entrenar segunda red con alpha=0.1\n",
    "model_2 = MLPClassifier(activation='relu', solver='adam', hidden_layer_sizes=(200, 300), alpha=0.1, random_state=123, max_iter=500)\n",
    "model_2.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones para la segunda red\n",
    "y_pred_2 = model_2.predict(X_test)\n",
    "\n",
    "# Calcular accuracy y matriz de confusión para la segunda red\n",
    "acc_2 = accuracy_score(y_test, y_pred_2)\n",
    "conf_matrix_2 = confusion_matrix(y_test, y_pred_2)\n",
    "\n",
    "# Guardar resultados de la segunda red\n",
    "results_2 = {\n",
    "    'Activation': 'relu',\n",
    "    'Solver': 'adam',\n",
    "    'Hidden Layer Sizes': (200, 300),\n",
    "    'Alpha': 0.1,\n",
    "    'Accuracy': acc_2,\n",
    "    'Confusion Matrix': conf_matrix_2\n",
    "}\n",
    "\n",
    "# Mostrar resultados en una tabla\n",
    "results_df = pd.DataFrame([results_1, results_2])\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>💟CODIGO COMPLETO💟</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Activation Solver Hidden Layer Sizes  Accuracy    Confusion Matrix\n",
      "0       relu   adam         (200, 300)  0.991228  [[73, 0], [1, 40]]\n",
      "1       tanh  lbfgs       (10, 20, 10)  0.938596  [[70, 3], [4, 37]]\n",
      "2   logistic    sgd       (20, 15, 10)  0.640351  [[73, 0], [41, 0]]\n",
      "3   identity   adam     (10, 10, 5, 5)  0.973684  [[73, 0], [3, 38]]\n",
      "4       tanh   adam          (150, 75)  0.964912  [[72, 1], [3, 38]]\n",
      "\n",
      "  Activation Solver Hidden Layer Sizes  Alpha  Accuracy    Confusion Matrix\n",
      "0       relu   adam         (200, 300)    0.3  0.964912  [[72, 1], [3, 38]]\n",
      "1       relu   adam         (200, 300)    0.1  0.982456  [[72, 1], [1, 40]]\n"
     ]
    }
   ],
   "source": [
    "# Importar bibliotecas necesarias\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# 1. Leer el archivo breastCancer.csv.\n",
    "data = pd.read_csv('breastCancer.csv')\n",
    "\n",
    "# 2. Seleccionar aleatoriamente el 80% del conjunto de datos para entrenar y el 20% restante para las pruebas.\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=123)\n",
    "\n",
    "# 3. Utilizar una estrategia para normalizar los datos.\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(train_data.iloc[:, :-1])  # Excluir la columna de diagnóstico\n",
    "y_train = train_data['diagnosis']\n",
    "\n",
    "X_test = scaler.transform(test_data.iloc[:, :-1])\n",
    "y_test = test_data['diagnosis']\n",
    "\n",
    "# 4. Construir 5 redes neuronales variando la función de activación, el solver, y la cantidad de capas ocultas y de neuronas por cada capa oculta.\n",
    "results = []\n",
    "\n",
    "# Seleccionar combinaciones específicas\n",
    "combinations = [\n",
    "    ('relu', 'adam', (200 , 300)),\n",
    "    ('tanh', 'lbfgs', (10, 20, 10)),\n",
    "    ('logistic', 'sgd', (20, 15, 10)),\n",
    "    ('identity', 'adam', (10, 10, 5, 5)),\n",
    "    ('tanh', 'adam', (150, 75)),\n",
    "]\n",
    "\n",
    "for activation, solver, hidden_layer_size in combinations:\n",
    "    # Crear y entrenar el modelo\n",
    "    model = MLPClassifier(activation=activation, solver=solver, hidden_layer_sizes=hidden_layer_size, random_state=123, max_iter=500)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Hacer predicciones\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calcular accuracy y matriz de confusión\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Guardar resultados\n",
    "    results.append({\n",
    "        'Activation': activation,\n",
    "        'Solver': solver,\n",
    "        'Hidden Layer Sizes': hidden_layer_size,\n",
    "        'Accuracy': acc,\n",
    "        'Confusion Matrix': conf_matrix\n",
    "    })\n",
    "\n",
    "# Mostrar resultados en una tabla\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "print(\"\")\n",
    "print(\"Se varia el hyperparametro de alpha\")\n",
    "\n",
    "#Variaciones con hyperparametro alpha\n",
    "\n",
    "# Entrenar primera red con alpha=0.3\n",
    "model_1 = MLPClassifier(activation='relu', solver='adam', hidden_layer_sizes=(200, 300), alpha=0.3, random_state=123, max_iter=500)\n",
    "model_1.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones para la primera red\n",
    "y_pred_1 = model_1.predict(X_test)\n",
    "\n",
    "# Calcular accuracy y matriz de confusión para la primera red\n",
    "acc_1 = accuracy_score(y_test, y_pred_1)\n",
    "conf_matrix_1 = confusion_matrix(y_test, y_pred_1)\n",
    "\n",
    "# Guardar resultados de la primera red\n",
    "results_1 = {\n",
    "    'Activation': 'relu',\n",
    "    'Solver': 'adam',\n",
    "    'Hidden Layer Sizes': (200, 300),\n",
    "    'Alpha': 0.3,\n",
    "    'Accuracy': acc_1,\n",
    "    'Confusion Matrix': conf_matrix_1\n",
    "}\n",
    "\n",
    "# Entrenar segunda red con alpha=0.1\n",
    "model_2 = MLPClassifier(activation='relu', solver='adam', hidden_layer_sizes=(200, 300), alpha=0.1, random_state=123, max_iter=500)\n",
    "model_2.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones para la segunda red\n",
    "y_pred_2 = model_2.predict(X_test)\n",
    "\n",
    "# Calcular accuracy y matriz de confusión para la segunda red\n",
    "acc_2 = accuracy_score(y_test, y_pred_2)\n",
    "conf_matrix_2 = confusion_matrix(y_test, y_pred_2)\n",
    "\n",
    "# Guardar resultados de la segunda red\n",
    "results_2 = {\n",
    "    'Activation': 'relu',\n",
    "    'Solver': 'adam',\n",
    "    'Hidden Layer Sizes': (200, 300),\n",
    "    'Alpha': 0.1,\n",
    "    'Accuracy': acc_2,\n",
    "    'Confusion Matrix': conf_matrix_2\n",
    "}\n",
    "\n",
    "# Mostrar resultados en una tabla\n",
    "results_df = pd.DataFrame([results_1, results_2])\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>💟TABLA DE RESULTADOS💟</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|💟Layer Sizes💟|💟Accuracy💟|💟Confusion Matrix💟|\n",
    "|--|--|--|\n",
    "|0     (200, 300) |0.991228| [73, 0], [1, 40]|\n",
    "|1     (10, 20, 10) | 0.938596 | [70, 3], [4, 37]|\n",
    "|2  (20, 15, 10) | 0.640351 | [73, 0], [41, 0]|\n",
    "|3  (10, 10, 5, 5) | 0.973684 | [73, 0], [3, 38]|\n",
    "|4  (150, 75) | 0.964912 | [72, 1], [3, 38]|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💟Hiperparámetros que por el momento le permiten obtener la red con mayor accuracy💟\n",
    "*   Con el solver \"adam\"\n",
    "*   Con una activacion \"relu\"\n",
    "*   Con un random state de 123\n",
    "\n",
    "Despues se modificó el hiperparámetro 💟alpha💟 \n",
    "\n",
    "Se modifico con 0.3 y 0.1 llegando a la conclusion de:\n",
    "\n",
    "💟El mejor accuracy lo tuvo en: 0.1💟"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
